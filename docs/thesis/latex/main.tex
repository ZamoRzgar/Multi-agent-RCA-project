\documentclass[12pt,a4paper]{report}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{fancyhdr}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue
}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    showstringspaces=false
}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Line spacing
\onehalfspacing

% Custom commands
\newcommand{\systemname}{Multi-Agent LLM}
\newcommand{\fone}{F\textsubscript{1}}

\begin{document}

%==============================================================================
% TITLE PAGE
%==============================================================================
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\LARGE\bfseries Multi-Agent Knowledge-Graph-Guided Reasoning\\for Reliable Log-Based Root Cause Analysis\par}
    
    \vspace{1.5cm}
    
    {\Large Master's Thesis\par}
    
    \vspace{2cm}
    
    {\large\itshape Experimental Report\par}
    
    \vspace{3cm}
    
    {\large December 2025\par}
    
    \vfill
    
    {\large \systemname{} Multi-Agent RCA System\par}
\end{titlepage}

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
Modern distributed systems generate massive volumes of log data critical for diagnosing failures and performing root cause analysis (RCA). While Large Language Models (LLMs) offer promising capabilities for automated log analysis, single-LLM approaches suffer from hallucinations, limited context, and lack of verification mechanisms.

This thesis presents \systemname{}, a multi-agent knowledge-graph-guided root cause analysis system. The system comprises six specialized agents: a Log Parser for structured extraction, a KG Retrieval Agent for historical context, three RCA Reasoners (Log-focused, KG-focused, and Hybrid), and a Judge Agent for hypothesis evaluation. These agents collaborate through a structured debate protocol enabling iterative hypothesis refinement and cross-validation.

We evaluate on the Hadoop1 dataset (55 labeled applications with 4 failure types) comparing against single-agent LLM and RAG (Retrieval-Augmented Generation) baselines. Results demonstrate that the multi-agent approach achieves: \textbf{+13.3 percentage points} improvement in coarse macro-\fone{} over single-agent (39.1\% vs 25.8\%), \textbf{+36.4 percentage points} improvement in disk\_full detection (36.4\% \fone{} vs 0\%), \textbf{81.0\% \fone{}} on connectivity-related failures, and \textbf{100\% convergence rate} in the debate protocol. The RAG baseline achieves 67.3\% coarse accuracy (highest) but lower macro-\fone{} than multi-agent (37.9\% vs 39.1\%), demonstrating that debate provides value beyond simple retrieval.

\textbf{Keywords:} Root Cause Analysis, Multi-Agent Systems, Large Language Models, Knowledge Graphs, Log Analysis, Distributed Systems
\end{abstract}

\tableofcontents
\listoftables
\listoffigures

%==============================================================================
% CHAPTER 1: INTRODUCTION
%==============================================================================
\chapter{Introduction}

\section{Problem Statement}

Modern distributed systems generate massive volumes of log data that are critical for understanding system behavior, diagnosing failures, and performing root cause analysis (RCA). As cloud-native architectures become increasingly complex---spanning microservices, container orchestration platforms, and distributed storage systems---the challenge of analyzing logs to identify the root cause of failures has grown exponentially.

Traditional approaches to log analysis rely heavily on manual inspection by system administrators or rule-based pattern matching systems. These methods suffer from several fundamental limitations:

\begin{enumerate}
    \item \textbf{Scale}: A single Hadoop cluster can generate millions of log entries per hour, making manual analysis infeasible.
    \item \textbf{Complexity}: Failures in distributed systems often manifest as cascading events across multiple components.
    \item \textbf{Domain Knowledge}: Effective RCA requires deep understanding of system architecture and historical patterns.
    \item \textbf{Timeliness}: Production incidents require rapid diagnosis, but manual analysis is inherently slow.
\end{enumerate}

\section{Research Questions}

This thesis investigates the following research questions:

\begin{description}
    \item[RQ1:] Does a multi-agent approach achieve higher accuracy than a single-agent LLM baseline for root cause analysis?
    \item[RQ2:] Does a structured debate protocol reduce hallucinations and improve reliability?
    \item[RQ3:] Does knowledge graph integration improve RCA quality?
    \item[RQ4:] Are the generated explanations high-quality and actionable?
\end{description}

\section{Contributions}

This thesis makes the following contributions:

\begin{enumerate}
    \item \textbf{System Architecture}: Design and implementation of \systemname{}, a multi-agent RCA system with six specialized agents.
    \item \textbf{Debate Protocol}: A multi-round debate protocol enabling iterative hypothesis refinement.
    \item \textbf{Knowledge Graph Integration}: A Neo4j knowledge graph storing historical incidents.
    \item \textbf{Empirical Evaluation}: Comprehensive evaluation comparing multi-agent vs single-agent approaches.
    \item \textbf{Open-Source Implementation}: Complete implementation using local LLMs via Ollama.
\end{enumerate}

%==============================================================================
% CHAPTER 2: SYSTEM DESIGN
%==============================================================================
\chapter{System Design and Architecture}

\section{System Overview}

\systemname{} is designed as a layered architecture with five distinct layers: Presentation, Orchestration, Agent, Knowledge, and Infrastructure.

\section{Agent Architecture}

All agents inherit from a common \texttt{BaseAgent} class. Table~\ref{tab:agents} summarizes the six agents.

\begin{table}[H]
\centering
\caption{Agent Summary}
\label{tab:agents}
\begin{tabular}{llll}
\toprule
\textbf{Agent} & \textbf{Model} & \textbf{Temperature} & \textbf{Purpose} \\
\midrule
Log Parser & qwen2:7b & 0.2 & Structured extraction \\
KG Retrieval & qwen2:7b & 0.5 & Historical context \\
Log Reasoner & mistral:7b & 0.7 & Log pattern analysis \\
KG Reasoner & llama2:7b & 0.7 & Historical reasoning \\
Hybrid Reasoner & qwen2:7b & 0.7 & Combined analysis \\
Judge & mistral:7b & 0.2 & Hypothesis evaluation \\
\bottomrule
\end{tabular}
\end{table}

\section{Knowledge Graph Design}

The knowledge graph uses a practical schema focused on incident-level analysis:

\begin{itemize}
    \item \textbf{Incident Node}: incident\_id, dataset, scenario\_id, final\_score, final\_hypothesis
    \item \textbf{Entity Node}: name, type (resource, config, component, issue)
    \item \textbf{RootCause Node}: description, confidence, source
\end{itemize}

\textbf{Relationships}:
\begin{itemize}
    \item \texttt{INVOLVES}: (Incident) $\rightarrow$ (Entity)
    \item \texttt{HAS\_ROOT\_CAUSE}: (Incident) $\rightarrow$ (RootCause)
    \item \texttt{SIMILAR\_TO}: (Incident) $\leftrightarrow$ (Incident)
\end{itemize}

\section{Debate Protocol}

The debate protocol orchestrates multi-round hypothesis refinement:

\begin{algorithm}[H]
\caption{Multi-Agent Debate Protocol}
\begin{algorithmic}[1]
\State \textbf{Input:} Parsed logs, KG context
\State \textbf{Output:} Final hypothesis with score
\For{round $= 1$ to max\_rounds}
    \State LogReasoner $\gets$ GenerateHypotheses(logs)
    \State KGReasoner $\gets$ GenerateHypotheses(kg\_context)
    \State HybridReasoner $\gets$ GenerateHypotheses(logs, kg\_context)
    \State all\_hypotheses $\gets$ Collect(LogReasoner, KGReasoner, HybridReasoner)
    \State scores, feedback $\gets$ Judge.Evaluate(all\_hypotheses)
    \If{Converged(scores)}
        \State \textbf{break}
    \EndIf
    \State Reasoners.Refine(feedback, other\_hypotheses)
\EndFor
\State \Return TopHypothesis(scores)
\end{algorithmic}
\end{algorithm}

%==============================================================================
% CHAPTER 3: EXPERIMENTAL SETUP
%==============================================================================
\chapter{Experimental Setup}

\section{Dataset: Hadoop1}

We evaluate on the \textbf{Hadoop1} dataset from LogHub, a widely-used benchmark for log analysis research.

\begin{table}[H]
\centering
\caption{Hadoop1 Dataset Statistics}
\label{tab:dataset}
\begin{tabular}{lrrl}
\toprule
\textbf{Label} & \textbf{Count} & \textbf{Percentage} & \textbf{Description} \\
\midrule
normal & 11 & 20.0\% & No injected fault \\
machine\_down & 28 & 50.9\% & Node failure injected \\
network\_disconnection & 7 & 12.7\% & Network partition injected \\
disk\_full & 9 & 16.4\% & Disk space exhaustion \\
\midrule
\textbf{Total} & \textbf{55} & \textbf{100\%} & \\
\bottomrule
\end{tabular}
\end{table}

\section{Evaluation Schemes}

We evaluate using both \textbf{strict} (4-class) and \textbf{coarse} (3-class) label schemes:

\begin{description}
    \item[Strict Labels (4-class):] normal, machine\_down, network\_disconnection, disk\_full
    \item[Coarse Labels (3-class):] normal, connectivity (= machine\_down $\cup$ network\_disconnection), disk\_full
\end{description}

\section{Evaluation Metrics}

\begin{enumerate}
    \item \textbf{Accuracy}: $\frac{\text{Correct Predictions}}{\text{Total Predictions}}$
    
    \item \textbf{Per-Class Precision}: $\text{Precision}_c = \frac{TP_c}{TP_c + FP_c}$
    
    \item \textbf{Per-Class Recall}: $\text{Recall}_c = \frac{TP_c}{TP_c + FN_c}$
    
    \item \textbf{Per-Class \fone{}}: $F1_c = 2 \cdot \frac{\text{Precision}_c \cdot \text{Recall}_c}{\text{Precision}_c + \text{Recall}_c}$
    
    \item \textbf{Macro-Averaged \fone{}}: $\text{Macro-}F1 = \frac{1}{|C|} \sum_{c \in C} F1_c$
\end{enumerate}

\section{Baseline Comparison}

We compare our multi-agent system against two baselines:
\begin{enumerate}
    \item \textbf{Single-Agent LLM}: A single LLM call with log context only, no KG retrieval or debate.
    \item \textbf{RAG (Retrieval-Augmented Generation)}: Single LLM call augmented with similar historical incidents retrieved from the Knowledge Graph.
\end{enumerate}

\begin{table}[H]
\centering
\caption{System Comparison}
\label{tab:systems}
\begin{tabular}{llll}
\toprule
\textbf{Aspect} & \textbf{Multi-Agent} & \textbf{RAG} & \textbf{Single-Agent} \\
\midrule
LLM Calls per app & 8--15 & 1 & 1 \\
Perspectives & 3 (Log, KG, Hybrid) & 1 & 1 \\
KG Integration & Yes & Yes (retrieval only) & No \\
Debate/Refinement & Yes (2--3 rounds) & No & No \\
Cross-Validation & Yes (Judge) & No & No \\
Runtime per app & 3--5 minutes & 30--45 seconds & 30 seconds \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
% CHAPTER 4: RESULTS
%==============================================================================
\chapter{Experimental Results}

\section{Multi-Agent Pipeline Results}

\subsection{Strict (4-class) Evaluation}

\begin{table}[H]
\centering
\caption{Multi-Agent Strict Evaluation Results}
\label{tab:ma-strict}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 21.8\% (12/55) \\
Macro Precision & 41.7\% \\
Macro Recall & 30.6\% \\
Macro \fone{} & 21.6\% \\
Unknown Predictions & 9 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Multi-Agent Per-Class Results (Strict)}
\label{tab:ma-strict-perclass}
\begin{tabular}{lrrrr}
\toprule
\textbf{Class} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{\fone{}} \\
\midrule
normal & 11 & 0.0\% & 0.0\% & 0.0\% \\
machine\_down & 28 & 50.0\% & 14.3\% & 22.2\% \\
network\_disconnection & 7 & 16.7\% & 85.7\% & 27.9\% \\
disk\_full & 9 & 100.0\% & 22.2\% & 36.4\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Multi-Agent Confusion Matrix (Strict)}
\label{tab:ma-cm-strict}
\begin{tabular}{l|ccccc}
\toprule
& \multicolumn{5}{c}{\textbf{Predicted}} \\
\textbf{Ground Truth} & normal & mach\_down & net\_disc & disk\_full & unknown \\
\midrule
normal (n=11) & 0 & 2 & 6 & 0 & 3 \\
machine\_down (n=28) & 0 & 4 & 21 & 0 & 3 \\
network\_disc (n=7) & 0 & 1 & 6 & 0 & 0 \\
disk\_full (n=9) & 0 & 1 & 3 & 2 & 3 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Coarse (3-class) Evaluation}

\begin{table}[H]
\centering
\caption{Multi-Agent Coarse Evaluation Results}
\label{tab:ma-coarse}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 61.8\% (34/55) \\
Macro Precision & 57.6\% \\
Macro Recall & 37.9\% \\
Macro \fone{} & 39.1\% \\
Unknown Predictions & 9 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Multi-Agent Per-Class Results (Coarse)}
\label{tab:ma-coarse-perclass}
\begin{tabular}{lrrrr}
\toprule
\textbf{Class} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{\fone{}} \\
\midrule
normal & 11 & 0.0\% & 0.0\% & 0.0\% \\
connectivity & 35 & 72.7\% & 91.4\% & 81.0\% \\
disk\_full & 9 & 100.0\% & 22.2\% & 36.4\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Multi-Agent Confusion Matrix (Coarse)}
\label{tab:ma-cm-coarse}
\begin{tabular}{l|cccc}
\toprule
& \multicolumn{4}{c}{\textbf{Predicted}} \\
\textbf{Ground Truth} & normal & connectivity & disk\_full & unknown \\
\midrule
normal (n=11) & 0 & 8 & 0 & 3 \\
connectivity (n=35) & 0 & 32 & 0 & 3 \\
disk\_full (n=9) & 0 & 4 & 2 & 3 \\
\bottomrule
\end{tabular}
\end{table}

\section{Single-Agent Baseline Results}

\subsection{Strict (4-class) Evaluation}

\begin{table}[H]
\centering
\caption{Single-Agent Strict Evaluation Results}
\label{tab:sa-strict}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 50.9\% (28/55) \\
Macro Precision & 13.2\% \\
Macro Recall & 25.0\% \\
Macro \fone{} & 17.3\% \\
Unknown Predictions & 0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Single-Agent Per-Class Results (Strict)}
\label{tab:sa-strict-perclass}
\begin{tabular}{lrrrr}
\toprule
\textbf{Class} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{\fone{}} \\
\midrule
normal & 11 & 0.0\% & 0.0\% & 0.0\% \\
machine\_down & 28 & 52.8\% & 100.0\% & 69.1\% \\
network\_disconnection & 7 & 0.0\% & 0.0\% & 0.0\% \\
disk\_full & 9 & 0.0\% & 0.0\% & 0.0\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Single-Agent Confusion Matrix (Strict)}
\label{tab:sa-cm-strict}
\begin{tabular}{l|cccc}
\toprule
& \multicolumn{4}{c}{\textbf{Predicted}} \\
\textbf{Ground Truth} & normal & mach\_down & net\_disc & disk\_full \\
\midrule
normal (n=11) & 0 & 10 & 0 & 1 \\
machine\_down (n=28) & 0 & 28 & 0 & 0 \\
network\_disc (n=7) & 0 & 6 & 0 & 1 \\
disk\_full (n=9) & 0 & 9 & 0 & 0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Coarse (3-class) Evaluation}

\begin{table}[H]
\centering
\caption{Single-Agent Coarse Evaluation Results}
\label{tab:sa-coarse}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 61.8\% (34/55) \\
Macro Precision & 21.4\% \\
Macro Recall & 32.4\% \\
Macro \fone{} & 25.8\% \\
Unknown Predictions & 0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Single-Agent Per-Class Results (Coarse)}
\label{tab:sa-coarse-perclass}
\begin{tabular}{lrrrr}
\toprule
\textbf{Class} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{\fone{}} \\
\midrule
normal & 11 & 0.0\% & 0.0\% & 0.0\% \\
connectivity & 35 & 64.2\% & 97.1\% & 77.3\% \\
disk\_full & 9 & 0.0\% & 0.0\% & 0.0\% \\
\bottomrule
\end{tabular}
\end{table}

\section{RAG Baseline Results}

The RAG (Retrieval-Augmented Generation) baseline retrieves similar historical incidents from the Knowledge Graph and includes them as context for the LLM prediction.

\subsection{Strict (4-class) Evaluation}

\begin{table}[H]
\centering
\caption{RAG Baseline Strict Evaluation Results}
\label{tab:rag-strict}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 49.1\% (27/55) \\
Macro Precision & 29.4\% \\
Macro Recall & 27.9\% \\
Macro \fone{} & 24.6\% \\
Unknown Predictions & 0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{RAG Baseline Per-Class Results (Strict)}
\label{tab:rag-strict-perclass}
\begin{tabular}{lrrrr}
\toprule
\textbf{Class} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{\fone{}} \\
\midrule
normal & 11 & 0.0\% & 0.0\% & 0.0\% \\
machine\_down & 28 & 51.0\% & 89.3\% & 64.9\% \\
network\_disconnection & 7 & 0.0\% & 0.0\% & 0.0\% \\
disk\_full & 9 & 66.7\% & 22.2\% & 33.3\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Coarse (3-class) Evaluation}

\begin{table}[H]
\centering
\caption{RAG Baseline Coarse Evaluation Results}
\label{tab:rag-coarse}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 67.3\% (37/55) \\
Macro Precision & 44.7\% \\
Macro Recall & 40.7\% \\
Macro \fone{} & 37.9\% \\
Unknown Predictions & 0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{RAG Baseline Per-Class Results (Coarse)}
\label{tab:rag-coarse-perclass}
\begin{tabular}{lrrrr}
\toprule
\textbf{Class} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{\fone{}} \\
\midrule
normal & 11 & 0.0\% & 0.0\% & 0.0\% \\
connectivity & 35 & 67.3\% & 100.0\% & 80.5\% \\
disk\_full & 9 & 66.7\% & 22.2\% & 33.3\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Comparative Analysis}

\begin{table}[H]
\centering
\caption{Comparative Summary: All Three Approaches}
\label{tab:comparison}
\begin{tabular}{lrrrr}
\toprule
\textbf{Metric} & \textbf{Multi-Agent} & \textbf{RAG} & \textbf{Single-Agent} & \textbf{Best} \\
\midrule
\multicolumn{5}{l}{\textit{Strict Evaluation (4-class)}} \\
Accuracy & 21.8\% & 49.1\% & 50.9\% & Single \\
Macro Precision & 41.7\% & 29.4\% & 13.2\% & \textbf{Multi} \\
Macro Recall & 30.6\% & 27.9\% & 25.0\% & \textbf{Multi} \\
Macro \fone{} & 21.6\% & 24.6\% & 17.3\% & \textbf{RAG} \\
\midrule
\multicolumn{5}{l}{\textit{Coarse Evaluation (3-class)}} \\
Accuracy & 61.8\% & 67.3\% & 61.8\% & \textbf{RAG} \\
Macro Precision & 57.6\% & 44.7\% & 21.4\% & \textbf{Multi} \\
Macro Recall & 37.9\% & 40.7\% & 32.4\% & \textbf{RAG} \\
Macro \fone{} & 39.1\% & 37.9\% & 25.8\% & \textbf{Multi} \\
\midrule
\multicolumn{5}{l}{\textit{Per-Class \fone{} (Coarse)}} \\
Normal & 0.0\% & 0.0\% & 0.0\% & --- \\
Connectivity & 81.0\% & 80.5\% & 77.3\% & \textbf{Multi} \\
Disk Full & 36.4\% & 33.3\% & 0.0\% & \textbf{Multi} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Accuracy is Misleading}: The single-agent baseline achieves higher strict accuracy (50.9\% vs 21.8\%) by exploiting class imbalance---it predicts \texttt{machine\_down} for almost all cases.
    
    \item \textbf{RAG Improves Over Single-Agent}: Adding KG retrieval to the single-agent baseline:
    \begin{itemize}
        \item Strict Macro-\fone{}: 24.6\% vs 17.3\% (+7.3 pp)
        \item Coarse Macro-\fone{}: 37.9\% vs 25.8\% (+12.1 pp)
        \item Enables \texttt{disk\_full} detection: 33.3\% \fone{} vs 0\%
    \end{itemize}
    
    \item \textbf{Multi-Agent Achieves Best Coarse Macro-\fone{}}: When all classes are weighted equally:
    \begin{itemize}
        \item Coarse Macro-\fone{}: Multi-Agent 39.1\% > RAG 37.9\% > Single 25.8\%
        \item Highest precision on minority classes (disk\_full: 100\% precision)
    \end{itemize}
    
    \item \textbf{All Systems Detect Minority Classes Differently}:
    \begin{itemize}
        \item \texttt{disk\_full}: Multi-Agent 36.4\% > RAG 33.3\% > Single 0\%
        \item \texttt{connectivity}: Multi-Agent 81.0\% > RAG 80.5\% > Single 77.3\%
    \end{itemize}
    
    \item \textbf{Debate Prevents Majority-Class Collapse}: The multi-agent system produces diverse predictions reflecting the actual class distribution, while RAG partially mitigates this through historical context.
\end{enumerate}

\section{Cross-Dataset Generalization}

\begin{table}[H]
\centering
\caption{Cross-Dataset Testing Results}
\label{tab:crossdataset}
\begin{tabular}{lrrrr}
\toprule
\textbf{Dataset} & \textbf{Scenarios} & \textbf{Avg Score} & \textbf{Convergence} & \textbf{Hybrid Win} \\
\midrule
HDFS & 3 & 91.7/100 & 100\% & 100\% \\
Hadoop & 3 & 91.0/100 & 100\% & 67\% \\
Spark & 3 & 90.7/100 & 100\% & 100\% \\
\midrule
\textbf{Overall} & \textbf{9} & \textbf{91.1/100} & \textbf{100\%} & \textbf{89\%} \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
% CHAPTER 5: DISCUSSION
%==============================================================================
\chapter{Discussion}

\section{Key Findings}

\subsection{Multi-Agent Debate Improves Classification Balance}

The multi-agent system produces more balanced predictions, as evidenced by the 13.3 percentage point improvement in coarse macro-\fone{}. This stems from:
\begin{itemize}
    \item \textbf{Preventing majority-class collapse}: Multiple reasoners generate diverse hypotheses
    \item \textbf{Cross-validation}: The judge evaluates evidence quality
    \item \textbf{Iterative refinement}: Feedback loops enable hypothesis improvement
\end{itemize}

\subsection{Minority Class Detection is the Key Differentiator}

The multi-agent system's ability to detect \texttt{disk\_full} (36.4\% \fone{} vs 0\%) represents the clearest advantage. This emerges from:
\begin{itemize}
    \item KG-focused reasoning leveraging historical patterns
    \item Hybrid integration combining log and historical context
    \item Evidence-based scoring rewarding specific, relevant evidence
\end{itemize}

\subsection{Symptom-Level vs Fault-Level Classification}

The gap between strict (21.8\%) and coarse (61.8\%) accuracy reveals that log-based RCA identifies \textit{symptoms}, not \textit{injected faults}. When a machine goes down, logs show connection failures---the same symptoms as network disconnection.

\subsection{Why RAG Achieves Highest Coarse Accuracy (67.3\%)}

The three systems achieve different coarse accuracies, revealing distinct behaviors:

\begin{itemize}
    \item \textbf{Single-Agent (61.8\%)}: Predicts \texttt{connectivity} for nearly all samples (53/55). This ``majority-class collapse'' yields moderate accuracy by chance but misses all disk\_full (0/9) and normal (0/11) cases.
    
    \item \textbf{RAG (67.3\%)}: The highest accuracy. KG retrieval provides historical context that helps the model make more informed predictions. It correctly classifies all 35 connectivity samples (100\% recall) while also detecting 2/9 disk\_full cases (22.2\% recall).
    
    \item \textbf{Multi-Agent (61.8\%)}: Achieves the same accuracy as single-agent but through more balanced predictions. It correctly classifies 32/35 connectivity samples, 2/9 disk\_full samples, and abstains on 9 cases (``unknown'').
\end{itemize}

The key insight is that \textbf{accuracy alone is misleading for imbalanced datasets}. While RAG achieves highest accuracy, the multi-agent system's superior macro-\fone{} (39.1\% vs 37.9\% vs 25.8\%) reflects its better balance across all classes, particularly its 100\% precision on disk\_full predictions.

\subsection{RAG vs Multi-Agent: When Does Debate Help?}

RAG and Multi-Agent both leverage the Knowledge Graph, but differ in how they use it:

\begin{itemize}
    \item \textbf{RAG}: Single LLM call with retrieved historical incidents as context. Fast (30-45 seconds) but no verification or refinement.
    
    \item \textbf{Multi-Agent}: Multiple reasoners generate hypotheses from different perspectives, then debate and refine through judge feedback. Slower (3-5 minutes) but produces higher-quality, verified predictions.
\end{itemize}

The multi-agent debate provides value when:
\begin{enumerate}
    \item High precision is required (disk\_full: 100\% vs 66.7\% precision)
    \item Evidence quality matters (judge scoring ensures well-supported hypotheses)
    \item Multiple failure modes may be present (diverse reasoner perspectives)
\end{enumerate}

\section{Limitations and Detailed Analysis}

\subsection{Dataset Scale Limitation}

The primary evaluation relies on the Hadoop1 dataset with only 55 labeled applications. While this is a standard benchmark from LogHub, the small sample size limits statistical power and generalizability. The class distribution is also imbalanced:

\begin{itemize}
    \item \texttt{machine\_down}: 28 samples (50.9\%)
    \item \texttt{normal}: 11 samples (20.0\%)
    \item \texttt{disk\_full}: 9 samples (16.4\%)
    \item \texttt{network\_disconnection}: 7 samples (12.7\%)
\end{itemize}

\textbf{Mitigation}: We report macro-averaged metrics to avoid bias toward majority classes, and we conducted cross-dataset testing on HDFS, Hadoop, and Spark (9 additional scenarios) to assess generalization.

\subsection{Normal Class Detection (0\% \fone{}): Concrete Analysis}

Both systems achieve 0\% \fone{} on the ``normal'' class. This is not a system failure but a \textbf{dataset labeling issue}. Examination of logs labeled as ``normal'' reveals they contain error patterns indistinguishable from failure cases:

\begin{lstlisting}[basicstyle=\ttfamily\scriptsize,frame=single,caption={Example log from ``normal'' labeled application}]
2015-10-18 18:15:09,449 WARN [main] org.apache.hadoop.hdfs.server.
    datanode.DataNode: Problem connecting to server: master:8020
2015-10-18 18:15:10,112 ERROR [IPC Client] connection to master
    failed on connection exception: java.net.ConnectException
2015-10-18 18:15:11,847 WARN [RPC] Retrying connect to server
\end{lstlisting}

These logs show connection errors, warnings, and retry attempts---patterns that any reasonable RCA system would classify as indicating a problem. The ``normal'' label in Hadoop1 means ``no fault was \textit{injected},'' not ``the system operated without errors.'' This semantic mismatch makes normal detection fundamentally ambiguous from logs alone.

\textbf{Recommendation}: Future work should incorporate external signals (job success/failure status, exit codes) or use datasets with cleaner normal/abnormal separation.

\subsection{Disk Full Recall (22.2\%): Detailed Analysis}

The multi-agent system detects disk\_full with 100\% precision but only 22.2\% recall (2/9 samples). Analysis of misclassified disk\_full cases reveals:

\begin{enumerate}
    \item \textbf{Symptom overlap}: Disk exhaustion often causes secondary connection failures as nodes become unresponsive, leading to connectivity-like log patterns.
    \item \textbf{Sparse disk-specific keywords}: Only 2/9 disk\_full applications contained explicit ``No space left on device'' messages in the sampled log lines.
    \item \textbf{Sampling limitation}: We sample up to 2500 lines per application; disk-related errors may appear in unsampled portions.
\end{enumerate}

\textbf{Recommendation}: Implement disk-specific keyword boosting in log sampling, or use a two-stage classifier (first detect disk keywords, then apply RCA).

\subsection{Baseline Comparison Scope}

The evaluation compares against two baselines: a single-agent LLM and a RAG (Retrieval-Augmented Generation) baseline. The RAG baseline demonstrates that KG retrieval alone provides significant improvement (+12.1 pp coarse macro-\fone{} over single-agent), while the multi-agent debate provides additional gains (+1.2 pp over RAG).

Additional baselines would provide more context for future work:

\begin{itemize}
    \item \textbf{Chain-of-Thought (CoT)}: Single LLM with explicit reasoning steps
    \item \textbf{Traditional ML}: Random Forest, SVM on log features (TF-IDF, n-grams)
    \item \textbf{Deep Learning}: DeepLog, LogAnomaly neural approaches
\end{itemize}

\subsection{Cross-Dataset Evaluation Metrics}

The cross-dataset evaluation (Table~\ref{tab:crossdataset}) uses different metrics than the Hadoop1 ground truth evaluation:

\begin{itemize}
    \item \textbf{Hadoop1 evaluation}: Classification accuracy, precision, recall, \fone{} against ground truth labels
    \item \textbf{Cross-dataset evaluation}: Judge scores (0-100), convergence rate, winner distribution
\end{itemize}

\textbf{Rationale}: The cross-dataset scenarios (HDFS, Hadoop, Spark) lack ground truth labels for failure types. Instead, we measure \textit{internal quality}---whether the debate protocol produces high-scoring, converged hypotheses. This complements the Hadoop1 evaluation, which measures \textit{external validity} against known labels.

The 91.1/100 average score and 100\% convergence rate demonstrate that the debate protocol consistently produces well-reasoned hypotheses, even if we cannot verify their correctness without labels.

\section{Threats to Validity}

\begin{description}
    \item[Internal Validity:] 
    \begin{itemize}
        \item Heuristic label mapping from free-text predictions to ground truth categories
        \item LLM output variability across runs (mitigated by low temperature for judge)
        \item Configuration sensitivity (model choice, temperature, max tokens)
    \end{itemize}
    
    \item[External Validity:]
    \begin{itemize}
        \item Single primary dataset (Hadoop1) limits generalizability
        \item Log format dependency---system may not transfer to non-Hadoop logs
        \item Scale---55 samples insufficient for production deployment claims
    \end{itemize}
    
    \item[Construct Validity:]
    \begin{itemize}
        \item Accuracy vs macro-\fone{} choice affects interpretation
        \item Coarse vs strict evaluation schemes yield different conclusions
        \item Ground truth quality---``normal'' labels are semantically ambiguous
    \end{itemize}
\end{description}

%==============================================================================
% CHAPTER 6: CONCLUSION
%==============================================================================
\chapter{Conclusion and Future Work}

\section{Summary of Contributions}

This thesis presented \systemname{}, demonstrating that multi-agent debate improves LLM-based RCA:

\begin{itemize}
    \item \textbf{+13.3 pp} coarse macro-\fone{} improvement
    \item \textbf{+36.4 pp} disk\_full detection improvement
    \item \textbf{81.0\%} connectivity \fone{}
    \item \textbf{100\%} debate convergence rate
\end{itemize}

\section{Answers to Research Questions}

\begin{description}
    \item[RQ1:] \textbf{Yes}, multi-agent achieves higher macro-\fone{} (+4.3 pp strict, +13.3 pp coarse)
    \item[RQ2:] \textbf{Yes}, debate protocol achieves 100\% convergence with iterative improvement
    \item[RQ3:] \textbf{Yes}, KG integration enables minority class detection
    \item[RQ4:] \textbf{Yes}, system produces specific, evidence-based explanations
\end{description}

\section{Future Work}

\begin{enumerate}
    \item \textbf{Additional Baselines}: Chain-of-Thought, traditional ML, deep learning approaches
    \item \textbf{Statistical Significance}: Bootstrap confidence intervals, McNemar tests
    \item \textbf{Ablation Studies}: Quantify component contributions
    \item \textbf{Knowledge Graph Expansion}: 50+ incidents, improved entity extraction
    \item \textbf{Normal Detection}: Dedicated healthy/unhealthy classifier
    \item \textbf{Real-Time Deployment}: Streaming analysis, incremental KG updates
\end{enumerate}

%==============================================================================
% REFERENCES
%==============================================================================
\chapter*{References}
\addcontentsline{toc}{chapter}{References}

\begin{enumerate}[label={[\arabic*]}]
 
    \item He, P., Zhu, J., Zheng, Z., and Lyu, M. R. (2017). Drain: An online log parsing approach with fixed depth tree. \textit{IEEE ICWS}.
    
    \item Du, M., Li, F., Zheng, G., and Srikumar, V. (2017). Deeplog: Anomaly detection and diagnosis from system logs through deep learning. \textit{ACM CCS}.
    
    \item Zhu, J., et al. (2019). Tools and benchmarks for automated log parsing. \textit{IEEE ICSE-SEIP}.
    
    \item Brown, T., et al. (2020). Language models are few-shot learners. \textit{NeurIPS}.
    
    \item Touvron, H., et al. (2023). Llama: Open and efficient foundation language models. \textit{arXiv:2302.13971}.
    
    \item Jiang, A. Q., et al. (2023). Mistral 7B. \textit{arXiv:2310.06825}.
    
    \item Wu, Q., et al. (2023). AutoGen: Enabling next-gen LLM applications via multi-agent conversation. \textit{arXiv:2308.08155}.
    
    \item Hogan, A., et al. (2021). Knowledge graphs. \textit{ACM Computing Surveys}.
    
    \item He, S., et al. (2020). Loghub: A large collection of system log datasets. \textit{arXiv:2008.06448}.
    
    \item Dang, Y., Lin, Q., and Huang, P. (2019). AIOps: Real-world challenges and research innovations. \textit{IEEE ICSE-Companion}.
    
    \item Notaro, P., Cardoso, J., and Gerndt, M. (2021). A survey of AIOps methods for failure management. \textit{ACM TIST}.
\end{enumerate}

\end{document}
