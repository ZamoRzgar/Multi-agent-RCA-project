Modern distributed systems generate massive volumes of logs that are essential for diagnosing failures and performing root cause analysis (RCA). While rule-based and statistical approaches can detect known patterns, they struggle to scale to heterogeneous cloud-native environments and often lack actionable explanations. Recent Large Language Models (LLMs) provide a new interface for log understanding and explanation generation; however, single-LLM RCA suffers from hallucinations, limited context, and the absence of verification.

This thesis presents \systemname{}, a multi-agent, knowledge-graph-guided RCA system that improves reliability through (i) specialization across multiple LLM agents and (ii) a structured debate protocol with an explicit judge. The system consists of six agents: a Log Parser for structured extraction, a Knowledge Graph (KG) Retrieval Agent for historical grounding, three RCA Reasoners (log-focused, KG-focused, and hybrid), and a Judge Agent that scores competing hypotheses on evidence quality, reasoning strength, confidence calibration, completeness, and consistency. The knowledge layer is implemented in Neo4j and stores incidents, entities, and root-cause relations to support retrieval and recurrence analysis.

We evaluate \systemname{} on three datasets: Hadoop1 from LogHub (55 labeled applications, 4 fault classes), CMCC from LogKG (93 industrial OpenStack failure cases, 7 classes), and HDFS\_v1 (200 balanced samples from 575,061 HDFS block traces, binary anomaly detection). On Hadoop1, \systemname{} improves coarse macro-\fone{} by +13.3 percentage points over a single-agent baseline (39.1\% vs 25.8\%) and detects minority faults such as disk\_full (36.4\% \fone{} vs 0\%). On CMCC, the multi-agent system achieves 61.3\% accuracy while the single-agent and RAG baselines collapse to ``unknown'' or majority-class predictions, providing strong evidence for cross-domain generalizability on real industrial logs. On HDFS\_v1, \systemname{} achieves 69.5\% accuracy and a balanced macro-\fone{} of 69.4\%, outperforming single-agent (65.0\%) and RAG (63.0\%) baselines.

Overall, the results support the hypothesis that multi-agent debate improves factuality and robustness by cross-validating hypotheses, reducing majority-class collapse, and producing evidence-grounded explanations. We discuss limitations such as label ambiguity (``normal'' semantics), dataset-specific KG coverage, and computational overhead, and we outline future work including larger-scale ablations, stronger retrieval, and additional ML/DL baselines.

\textbf{Key Words:} Root Cause Analysis, Log Analysis, Large Language Models, Multi-Agent Systems, Knowledge Graphs, Debate, AIOps
