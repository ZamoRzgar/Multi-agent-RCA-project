\section{Overview}
This chapter describes the experimental methodology used to evaluate \systemname{}. We detail the evaluation protocol, label normalization procedures, baseline configurations, and the workflow for running large-scale experiments.

The methodology is designed to support reproducible evaluation of LLM-based RCA systems. Unlike traditional ML pipelines that require training and validation splits, our system operates in a zero-shot or few-shot setting: the LLMs are not fine-tuned on the target datasets, and the knowledge graph is populated incrementally as incidents are processed.

\section{Evaluation Protocol}

\subsection{End-to-End Pipeline}
For each incident in a dataset, the evaluation proceeds as follows:
\begin{enumerate}
  \item \textbf{Log ingestion}: raw log files are loaded and associated with ground-truth labels.
  \item \textbf{Parsing}: the Log Parser Agent extracts structured events, entities, and error messages.
  \item \textbf{KG retrieval}: the KG Retrieval Agent queries for similar historical incidents (if any exist in the KG).
  \item \textbf{Hypothesis generation}: the three reasoners generate competing hypotheses.
  \item \textbf{Judging}: the Judge Agent scores hypotheses and selects the best one.
  \item \textbf{Label normalization}: the free-text hypothesis is mapped to a dataset label.
  \item \textbf{Recording}: predictions, scores, and metadata are logged for analysis.
\end{enumerate}

This pipeline is executed for all incidents in the dataset. The KG is updated after each incident, so later incidents may benefit from historical context accumulated from earlier ones. However, for fair evaluation, we do not use ground-truth labels during retrieval---only the system's own predictions and scores.

\subsection{Baseline Configurations}
We compare three pipeline configurations:
\begin{description}
  \item[Multi-Agent:] the full system with three reasoners, judge, and debate protocol (8--15 LLM calls per incident, 3--5 minutes runtime).
  \item[RAG:] a single LLM call augmented with retrieved historical context from the KG (1 LLM call, 30--45 seconds runtime).
  \item[Single-Agent:] a single LLM call with log context only, no KG retrieval or debate (1 LLM call, ~30 seconds runtime).
\end{description}

All configurations use the same underlying models and prompting style for fair comparison. The key difference is the amount of computation and verification applied to each incident.

\section{Label Normalization}

\subsection{The Normalization Problem}
LLM-based RCA systems produce free-text hypotheses (e.g., ``The root cause is a network partition between nodes A and B''). To evaluate against ground-truth labels, we must map these hypotheses to the dataset's label space (e.g., \texttt{network\_disconnection}).

This mapping is non-trivial because:
\begin{itemize}
  \item hypotheses may use different terminology than the dataset labels,
  \item hypotheses may describe symptoms rather than root causes,
  \item hypotheses may be ambiguous or span multiple categories.
\end{itemize}

\subsection{Conservative Mapping Rules}
We implement a conservative rule-based normalizer that:
\begin{enumerate}
  \item searches for exact or near-exact matches to label keywords,
  \item applies dataset-specific synonym mappings (e.g., ``node failure'' $\rightarrow$ \texttt{machine\_down}),
  \item defaults to \texttt{unknown} when no confident match is found.
\end{enumerate}

The conservative approach is designed to avoid over-interpreting ambiguous text. This means some correct diagnoses may be marked as \texttt{unknown} if they do not use expected keywords, but it reduces the risk of false positives from aggressive pattern matching.

\subsection{Strict vs. Coarse Label Schemes}
For Hadoop1, we evaluate under two label schemes:
\begin{description}
  \item[Strict (4-class):] \texttt{normal}, \texttt{machine\_down}, \texttt{network\_disconnection}, \texttt{disk\_full}.
  \item[Coarse (3-class):] \texttt{normal}, \texttt{connectivity} (= \texttt{machine\_down} $\cup$ \texttt{network\_disconnection}), \texttt{disk\_full}.
\end{description}

The coarse scheme merges semantically similar connectivity-related faults, which better reflects the symptom-level evidence visible in logs. This is important because log messages often describe connectivity symptoms without distinguishing whether the underlying cause is a machine failure or a network partition.

\section{Metrics}

\subsection{Primary Metrics}
We report the following metrics:
\begin{itemize}
  \item \textbf{Accuracy}: fraction of incidents where the predicted label matches ground truth.
  \item \textbf{Macro Precision}: unweighted mean of per-class precision.
  \item \textbf{Macro Recall}: unweighted mean of per-class recall.
  \item \textbf{Macro \fone{}}: unweighted mean of per-class \fone{} scores.
\end{itemize}

\subsection{Why Macro Metrics Matter}
In operational RCA, rare failure types are often the most critical to diagnose correctly. Macro-averaged metrics give equal weight to each class regardless of frequency, which better reflects diagnostic utility than accuracy alone.

For example, a system that always predicts the majority class may achieve high accuracy but zero recall on minority classes. Macro \fone{} penalizes this behavior by averaging across all classes.

\subsection{Unknown Predictions}
When the normalizer cannot confidently map a hypothesis to a label, it outputs \texttt{unknown}. We report the number of unknown predictions separately because:
\begin{itemize}
  \item a high unknown rate may indicate poor hypothesis quality or overly conservative normalization,
  \item unknown predictions are counted as incorrect for accuracy but do not contribute to per-class metrics.
\end{itemize}

\section{Experimental Workflow}

\subsection{Infrastructure}
All experiments are run on a local machine with:
\begin{itemize}
  \item LLM inference via Ollama (qwen2:7b, mistral:7b, llama2:7b),
  \item Neo4j for knowledge graph storage and querying,
  \item Python orchestration scripts for pipeline execution and result collection.
\end{itemize}

This local setup ensures reproducibility and avoids dependence on external API availability or rate limits.

\subsection{Reproducibility Considerations}
LLM outputs are inherently stochastic due to sampling. We mitigate variability by:
\begin{itemize}
  \item using low temperatures (0.2) for structured extraction and judging,
  \item using moderate temperatures (0.7) for reasoning to encourage diversity,
  \item logging all prompts, responses, and intermediate outputs for post-hoc analysis.
\end{itemize}

Despite these measures, some variability remains. We report single-run results in this thesis; future work could explore multi-run aggregation for more robust estimates.

\subsection{Runtime Characteristics}
Table~\ref{tab:runtime} summarizes typical runtime characteristics per incident.

\begin{table}[H]
\centering
\caption{Runtime Characteristics per Incident}
\label{tab:runtime}
\begin{tabular}{lrrr}
\toprule
\textbf{Pipeline} & \textbf{LLM Calls} & \textbf{Runtime} & \textbf{KG Queries} \\
\midrule
Multi-Agent & 8--15 & 3--5 min & 1--3 \\
RAG & 1 & 30--45 sec & 1 \\
Single-Agent & 1 & ~30 sec & 0 \\
\bottomrule
\end{tabular}
\end{table}

The multi-agent pipeline is significantly more expensive than baselines, but this cost is justified by improved diagnostic quality in cross-domain and imbalanced settings.

\section{Scope and Limitations of the Methodology}

\subsection{ML/DL Baselines}
We do not include supervised ML/DL baselines (e.g., DeepLog, LogAnomaly, LogBERT) in our main comparison because:
\begin{itemize}
  \item they require dataset-specific training and hyperparameter tuning,
  \item they use different preprocessing and feature pipelines,
  \item our goal is to evaluate explanation-oriented, zero-shot LLM systems.
\end{itemize}

Comparison with trained ML/DL baselines is deferred to future work.

\subsection{Single-Run Evaluation}
Due to computational cost, we report single-run results. This is a limitation because LLM outputs can vary across runs. However, our use of low temperatures for critical components and the multi-agent design (which aggregates across multiple reasoners) helps reduce sensitivity to individual sampling decisions.

\subsection{Label Normalization Errors}
The rule-based normalizer may introduce errors when hypotheses use unexpected terminology. This is a known limitation of evaluating free-text outputs against discrete labels. We mitigate this by using conservative rules and reporting unknown rates.
